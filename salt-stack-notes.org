Jeffs notes on using Salt
* Targeting nodes
There are several ways to name which nodes to affect.  As well, these can be combined using a _type_ ~@~ prefix with boolean operators.  Eg:
#+BEGIN_SRC bash
salt -C 'G@os:Ubuntu and I@role:web and S@192.168.100.0/24' test.ping
#+END_SRC
Note the use of ~-C~ for Compound targeting

Here is the table of type target types and what they affect
| Prefix | Target type                   |
|--------+-------------------------------|
| G      | Grains                        |
| E      | PCRE over minion id           |
| P      | PCRE over grains              |
| L      | explicit comma separated list |
| I      | Pillar                        |
| J      | PCR over Pillar               |
| S      | Subnet / IPv4 CIDR            |
| R      | SECO range                    |

** batch size
Additionally, one can control how many of the matched hosts are operated on at a time using ~--batch-size~, which can be an integer or a %.

Here 10 machines at a time are pinged
#+BEGIN_SRC bash
salt '*' -b 10 test.ping
#+END_SRC
and here we restart the web servers in a rolling 25% window
#+BEGIN_SRC bash
salt -G 'os:RedHat' --batch-size 25% apache.signal restart
#+END_SRC

** Nodegroups
Alternatively, one can configure named sets of nodes in the master config file.  As an example:
#+BEGIN_SRC yaml
nodegroups:
    webdev:  'I@role:web and G:cluster:dev'
    webqa:   'I@role:web and G:cluster:qa'
    webprod: 'I@role:web and G:cluster:prod'
#+END_SRC
Then use ~-N~ to target these nodes
#+BEGIN_SRC bash
salt -N webqa test.ping
#+END_SRC
* Dependencies with require, prereq
Here is an example that ensures the config file is installed before the service gets installed (and started by the OS).
#+BEGIN_SRC salt
apache2:
  pkg:
    - installed
    - require:
      - file: apache2
  service:
    - running
    - require:
      - pkg: apache2
  file:
    - managed
    - name /etc/apache2/apache2.conf
    - source: salt://apache2/apache2.conf
#+END_SRC
and here is an example that will shutdown and then restart apache if the codebase it is serving changes
#+BEGIN_SRC salt
apache2:
  service:
    - running
    - watch:
        file: codebase

codebase:
  - file:
    - recurse
    # etc

shutdown_apache2:
  service:
    - dead
    - name: apache2
    - prereq:
      - file: codebase
#+END_SRC
When evaluating the /shutdown_apache2/ state, salt will follow the /prereq/ clause.
If the codebase production triggers an update, then the shutdown_apache production will be executed before the codebase production.
Apache will be restarted by the watch on apache2.

The ~extend~ block is often used to specialize generic servicetemplates.
* Grains, pillars and templates
Grains are defined by the minion; they are specific to the minion.
Pillars are defined on the server.
Either can be defined statically or dynamically.
However, in practice, grains are more typically used for info that is not likely to change.
Pillars tend to be more dynamic info.

** Grains
Grains are loaded at minion startup, and cached in RAM.

To see the grains on minion, use ~grains.items~
#+BEGIN_SRC bash
salt myminion grains.items
#+END_SRC
and to see a single grain
#+BEGIN_SRC sh
salt myminion grains.item os_family
#+END_SRC

Grains are typically stored in ~/etc/salt/grains~ on a minion.
This file is re-written by salt periodically.

To add or modify a value in the grains file, use ~setval~
#+BEGIN_SRC bash
salt myminion grains.setval mygrain 42
#+END_SRC

There is also ~grains.append~ to add to a grain that is a list, and ~grains.delval~ to delete.
** Pillars
Pillars are server side, and are typically in ~/srv/pillar/~.
~pillar.items~ will show all of the pillar data.

By default, minions can only see the pillar state they are configured for.
Ie, minions cannot reference pillar state they do not participate in.
