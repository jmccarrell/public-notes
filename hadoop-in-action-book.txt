multiset: a set where each element also has a count.
  typically implemented as a hash.
  word count is the canonical example of a multiset.

bigram: a pair of consecutive words
trigram: 3 consecutive words

the sentence: "Do as I say, not as I do" has bigrams:
Do as
as I
I say
say not
not as
I do

The input format for processing documents is often
list(<string file_name, string file_content>)

Often the input format for logfiles is:
list(<integer line_number, string log_line>)

Hadoop data types

values must implement java's Writable interface.
keys must implement WritableComparable<T>, which is a combination of Writable and Comparable<T>
since keys have to be sorted while values are simply passed through.

Basic type wrapper classes:

BooleanWritable         Wrapper for boolean
ByteWritable                        single byte
DoubleWritable
FloatWritable
IntWritable
LongWritable
Text                                store text using UTF8
NullWritable            Placeholder when the key or value is not needed


An implementation of a Comparable<T> class:
(an edge to represent a graph)

public class Edge implements WritableComparable<Edge> {
    private string departureNode;
    private string arrivalNode;

    @Override
    public void readFields(DataInput in) throws IOException {
        departureNode = in.readUTF();
        arrivalNode   = in.readUTF();
    }

    @Override
    public void writeFields(DataInput out) throws IOException {
        out.readUTF(departureNode);
        out.readUTF(arrivalNode);
    }

    @Override
    public int compareTo(Edge o) {
        return (departureNode.compareTo(o.departureNode) != 0)
            ? departureNode.compareTo(o.departureNode)
            : arrivalNode.compareTo(o.arrivalNode);
    }
}

The mapper discussion in the book is for the older API
(explanation why in section 4.4).
The code in the word count examples have it right, so I should use
org.apache.hadoop.mapreduce
not
org.apache.hadoop.mapred

The generic link for the 0.20 docs is at:
  http://hadoop.apache.org/common/docs/r0.20.0/api/index.html

The package summary for o.a.h.mapreduce.Mapper is at:
http://hadoop.apache.org/common/docs/r0.20.0/api/org/apache/hadoop/mapreduce/package-summary.html

Mapper provides setup, cleanup and run methods, as well as map.

    The Hadoop Map-Reduce framework spawns one map task for each InputSplit generated by
    the InputFormat for the job. Mapper implementations can access the Configuration for
    the job via the JobContext.getConfiguration().

    The framework first calls setup(org.apache.hadoop.mapreduce.Mapper.Context), followed
    by map(Object, Object, Context) for each key/value pair in the InputSplit. Finally
    cleanup(Context) is called.

    All intermediate values associated with a given output key are subsequently grouped by
    the framework, and passed to a Reducer to determine the final output. Users can
    control the sorting and grouping by specifying two key RawComparator classes.

    The Mapper outputs are partitioned per Reducer. Users can control which keys (and
    hence records) go to which Reducer by implementing a custom Partitioner.

    Users can optionally specify a combiner, via Job.setCombinerClass(Class), to perform
    local aggregation of the intermediate outputs, which helps to cut down the amount of
    data transferred from the Mapper to the Reducer.

    Applications can specify if and how the intermediate outputs are to be compressed and
    which CompressionCodecs are to be used via the Configuration.

    If the job has zero reduces then the output of the Mapper is directly written to the
    OutputFormat without sorting by keys.

Mapper provides implementations:
  (which are in o.a.h.mapreduce.lib.map)
  InverseMapper
  MultithreadedMapper
  TokenCounterMapper

as well, the default map function of o.a.h.mapreduce.Mapper is the identity function.


Reducer
The reducer summary page says, among other things:
  http://hadoop.apache.org/common/docs/r0.20.0/api/org/apache/hadoop/mapreduce/Reducer.html

    SecondarySort

    To achieve a secondary sort on the values returned by the value iterator, the
    application should extend the key with the secondary key and define a grouping
    comparator. The keys will be sorted using the entire key, but will be grouped using
    the grouping comparator to decide which keys and values are sent in the same call to
    reduce.The grouping comparator is specified via
    Job.setGroupingComparatorClass(Class). The sort order is controlled by
    Job.setSortComparatorClass(Class).

    For example, say that you want to find duplicate web pages and tag them all with the
    url of the "best" known example. You would set up the job like:

    Map Input Key: url
    Map Input Value: document
    Map Output Key: document checksum, url pagerank
    Map Output Value: url
    Partitioner: by checksum
    OutputKeyComparator: by checksum and then decreasing pagerank
    OutputValueGroupingComparator: by checksum

o.a.h.mapreduce.lib.Reducer implements: 
IntSumReducer
LongSumReducer


Job
the Job class is where the mechanisms are set up, via methods like:

    setCombinerClass
    setPartitionerClass
    setGroupingComparatorClass
    setSortComparatorClass

and others.

InputFormat

    InputFormat describes the input-specification for a Map-Reduce job.

    The Map-Reduce framework relies on the InputFormat of the job to:

      1. Validate the input-specification of the job.

      2. Split-up the input file(s) into logical InputSplits, each of which is then
         assigned to an individual Mapper.

      3. Provide the RecordReader implementation to be used to glean input records from
         the logical InputSplit for processing by the Mapper.


    The default behavior of file-based InputFormats, typically sub-classes of
    FileInputFormat, is to split the input into logical InputSplits based on the total
    size, in bytes, of the input files. However, the FileSystem blocksize of the input
    files is treated as an upper bound for input splits. A lower bound on the split size
    can be set via mapred.min.split.size.

    Clearly, logical splits based on input-size is insufficient for many applications
    since record boundaries are to respected. In such cases, the application has to also
    implement a RecordReader on whom lies the responsibility to respect record-boundaries
    and present a record-oriented view of the logical InputSplit to the individual task.

    See Also:
      InputSplit, RecordReader, FileInputFormat

